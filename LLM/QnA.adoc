항목 - 현재 기술과 비교
언어와 이미지의 공통 처리 - 맞음. 모두 벡터 공간으로 변환해 계산.
인간 두뇌와 AI 처리 유사성 - 부분적으로 유사하지만, AI는 확률적 패턴 매칭.
언어·이미지 좌표 공간의 공통성 - 맞음. 멀티모달 모델이 이 원리로 작동.
외국어 질의 필요성 - 부분적으로 맞음. 학습 언어 비중이 중요.
Own LLM 필요성 - 특수 도메인이나 문화적 특화 작업에서 필요.


가설
평가 및 코멘트
가설1: 같은 질문 → 같은 속도의 같은 대답
부분적으로만 맞음. LLM은 확률적 모델이라 temperature 같은 설정이나 서버 상황에 따라 응답이 조금씩 달라질 수 있음.
가설2: 의미가 같은 질문 → 같은 대답
부분적으로 맞음. 의미가 같으면 유사한 벡터 임베딩으로 비슷한 답이 나오지만, 표현이 미묘하게 달라지기도 함.
가설3: 색상코드가 달라도 의미는 같음
맞음. 의미론적으로 같은 입력은 유사하게 처리되지만, 토큰화 수준에서 차이가 있으면 출력이 달라질 수 있음.
가설4: 사투리/은어 구분
맞음. LLM은 텍스트를 토큰으로 처리하기 때문에, 사투리도 토큰화가 되면 유사하게 이해 가능. 다만 학습 데이터에 따라 성능 차이가 있음.
가설5: 정박·엇박 차이만 있는 노래 구분
실험 필요. 음악 분석은 오디오 기반 모델이 필요하며, 텍스트 기반 모델만으로는 한계가 큼.
가설6: 창의적 문제 해결(가위바위보 게임)
좋은 실험 아이디어. 명확히 알고리즘화 가능. 추론·코딩·확률 계산을 복합적으로 테스트 가능.
가설7: 기능 추가 시 재사용 여부
유효한 가설. 동일 기능을 여러 맥락에서 재활용할 수 있는지, 프롬프트 엔지니어링·코드 모듈화로 실험 가능.


토큰 모델이 텍스트를 처리하기 위해 잘게 쪼갠 단위. 영어는 단어·어절 단위, 한국어는 형태소 또는 음절 단위로 쪼개짐.
인코딩/디코딩
인코딩: 문자열을 숫자 벡터로 변환해 모델 입력으로 사용.
디코딩: 모델이 생성한 숫자 벡터를 다시 문자로 변환.
해시·암호화 개념과 혼동 주의
토큰 인코딩은 단순 변환일 뿐 보안 암호화와 다름.

저맥락 vs 고맥락
저맥락: 이전 대화나 많은 맥락 없이 단일 요청에 반응.
고맥락: 긴 대화 이력이나 대량 문서를 참조.

원문
정정
토큰 = “압축/인코딩된 문자”
토큰은 압축이라기보다 단위화한 문자열 조각.
인코딩/디코딩 = 해시 암호화/복호화
다른 개념. LLM 인코딩은 단순한 숫자 변환이지 보안 암호화가 아님.

모델
분석 내용
평가 및 정정

ChatGPT
연산 + 참조, 메모리 사용
부분적으로 맞음. ChatGPT는 연산(추론)과 학습된 파라미터 참조를 기반으로 응답함. 최근에는 일부 대화 맥락을 저장하는 메모리 기능을 지원하지만, 모든 대화에 상시 적용되진 않음.

Claude
연산 중심
대체로 맞음. Claude는 현재까지 “메모리 저장 기능”이 거의 없고, 연산 기반 추론에 집중.

Gemini

Gemini는 구글의 멀티모달 모델로, 연산+참조 구조는 유사하나, Google Search와 연계한 참조 기능이 상대적으로 강점.

Grok

X의 AI 모델로, 최신 대화·검색 데이터와 결합 가능.

Perplexity

검색 기반 **RAG(Retrieval-Augmented Generation)**이 강점. 순수 생성형이라기보다는 검색+생성 모델.

DeepSeek

주로 오픈소스 계열로, 효율적 추론을 지향하는 모델.

Copilot

사실상 GPT나 다른 LLM을 IDE에 통합한 서비스. 모델 자체보다는 프론트엔드 역할이 큼.
